#!/usr/bin/env python3
"""
Get job listings from LinkedIn company pages - improved version.
Uses more robust selectors and page inspection.
"""

import sys
import os
import json
import time
from datetime import datetime
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'core', 'mcp'))

from linkedin_server import sync_playwright, _get_browser_context, _wait_for_linkedin_load, _is_logged_in, _save_context_state

def get_company_jobs(page, company_url):
    """Extract job listings from company LinkedIn page Jobs tab."""
    jobs = []
    
    try:
        # Go directly to jobs page
        if company_url.endswith('/'):
            jobs_url = company_url + 'jobs/'
        else:
            jobs_url = company_url + '/jobs/'
        
        print(f"  –û—Ç–∫—Ä—ã–≤–∞—é: {jobs_url}")
        page.goto(jobs_url)
        _wait_for_linkedin_load(page)
        time.sleep(5)  # Wait longer for dynamic content
        
        # Take screenshot for debugging (optional)
        # page.screenshot(path=f"/tmp/jobs_{company_url.split('/')[-2]}.png")
        
        # Try multiple strategies to find jobs
        found_jobs = []
        
        # Strategy 1: Look for job card containers
        job_containers = page.locator('div[class*="job-card"], li[class*="job"], div[data-test-id*="job"]').all()
        print(f"  –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ —Å job: {len(job_containers)}")
        
        if len(job_containers) > 0:
            for container in job_containers[:30]:  # Limit to 30
                try:
                    # Try to find title/link
                    title_elem = container.locator('h3, h4, a[href*="/jobs/view/"], span[class*="title"]').first
                    if title_elem.count() > 0:
                        title = title_elem.inner_text().strip()
                        
                        # Find link
                        link_elem = container.locator('a[href*="/jobs/view/"]').first
                        job_url = None
                        if link_elem.count() > 0:
                            href = link_elem.get_attribute('href')
                            if href:
                                if not href.startswith('http'):
                                    job_url = f"https://www.linkedin.com{href}"
                                else:
                                    job_url = href
                        
                        # Find location
                        location = ""
                        location_selectors = [
                            'span[class*="location"]',
                            'span[class*="job-location"]',
                            'div[class*="location"]',
                        ]
                        for loc_sel in location_selectors:
                            loc_elem = container.locator(loc_sel).first
                            if loc_elem.count() > 0:
                                location = loc_elem.inner_text().strip()
                                break
                        
                        if title and len(title) > 3:  # Valid title
                            found_jobs.append({
                                'title': title,
                                'url': job_url or '',
                                'location': location,
                            })
                except Exception as e:
                    continue
        
        # Strategy 2: Look for all job links on page
        if len(found_jobs) == 0:
            print("  –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –ø—Ä–æ–±—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é 2...")
            job_links = page.locator('a[href*="/jobs/view/"]').all()
            print(f"  –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏: {len(job_links)}")
            
            seen_titles = set()
            for link in job_links[:30]:
                try:
                    href = link.get_attribute('href')
                    if href:
                        if not href.startswith('http'):
                            job_url = f"https://www.linkedin.com{href}"
                        else:
                            job_url = href
                        
                        title = link.inner_text().strip()
                        if not title:
                            # Try to get title from parent or nearby
                            parent = link.locator('..').first
                            title = parent.inner_text().strip()[:100]
                        
                        if title and len(title) > 3 and title not in seen_titles:
                            seen_titles.add(title)
                            found_jobs.append({
                                'title': title,
                                'url': job_url,
                                'location': '',
                            })
                except Exception:
                    continue
        
        # Strategy 3: Parse page HTML for job data
        if len(found_jobs) == 0:
            print("  –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2 –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –ø—Ä–æ–±—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é 3...")
            page_content = page.content()
            import re
            
            # Look for job IDs in URLs
            job_ids = re.findall(r'/jobs/view/(\d+)', page_content)
            print(f"  –ù–∞–π–¥–µ–Ω–æ ID –≤–∞–∫–∞–Ω—Å–∏–π –≤ HTML: {len(set(job_ids))}")
            
            for job_id in list(set(job_ids))[:20]:
                job_url = f"https://www.linkedin.com/jobs/view/{job_id}/"
                # Try to find title near this ID
                pattern = rf'jobs/view/{job_id}[^>]*>([^<]+)'
                matches = re.findall(pattern, page_content)
                title = matches[0].strip() if matches else f"Job {job_id}"
                
                found_jobs.append({
                    'title': title,
                    'url': job_url,
                    'location': '',
                })
        
        return {
            'success': True,
            'company_url': company_url,
            'jobs_count': len(found_jobs),
            'jobs': found_jobs
        }
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return {
            'success': False,
            'company_url': company_url,
            'error': str(e),
            'error_details': error_details,
            'jobs': []
        }

# Main execution
if __name__ == "__main__":
    companies = [
        "https://www.linkedin.com/company/barcrest-games",
        "https://www.linkedin.com/company/epicwinglobal",
    ]
    
    print("üîç –°–æ–±–∏—Ä–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)...\n")
    
    all_results = []
    
    with sync_playwright() as playwright:
        context = _get_browser_context(playwright, headless=False)
        page = context.new_page()
        
        try:
            # Check login
            page.goto('https://www.linkedin.com/feed')
            _wait_for_linkedin_load(page)
            
            if not _is_logged_in(page):
                print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –∑–∞–ª–æ–≥–∏–Ω–µ–Ω –≤ LinkedIn")
                sys.exit(1)
            
            print("‚úÖ –í—Ö–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω\n")
            
            for i, company_url in enumerate(companies, 1):
                print(f"[{i}/{len(companies)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {company_url}")
                result = get_company_jobs(page, company_url)
                all_results.append(result)
                
                if result['success']:
                    print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {result['jobs_count']}")
                    if result['jobs_count'] > 0:
                        for j, job in enumerate(result['jobs'][:3], 1):
                            print(f"    {j}. {job['title']}")
                else:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞: {result.get('error', 'Unknown')}")
                
                if i < len(companies):
                    time.sleep(3)
            
            _save_context_state(context)
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
        finally:
            context.close()
    
    # Save results
    vault_path = os.environ.get("VAULT_PATH", os.path.dirname(os.path.dirname(__file__)))
    output_file = os.path.join(vault_path, "00-Inbox", "Job_Search", "debug", f"linkedin-jobs-failed-companies-{datetime.now().strftime('%Y-%m-%d')}.md")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # Generate markdown digest
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# –í–∞–∫–∞–Ω—Å–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π (–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è)\n\n")
        f.write(f"**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
        f.write(f"–ö–æ–º–ø–∞–Ω–∏–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –≤ LinkedIn, –∏ –∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏.\n\n")
        f.write("---\n\n")
        
        for result in all_results:
            company_name = result['company_url'].split('/company/')[-1].replace('-', ' ').title()
            f.write(f"## {company_name}\n\n")
            f.write(f"**LinkedIn:** {result['company_url']}\n\n")
            
            if result['success']:
                if result['jobs_count'] > 0:
                    f.write(f"**–ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π:** {result['jobs_count']}\n\n")
                    f.write("### –û—Ç–∫—Ä—ã—Ç—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏:\n\n")
                    for job in result['jobs']:
                        f.write(f"- **{job['title']}**")
                        if job['location']:
                            f.write(f" - {job['location']}")
                        f.write("\n")
                        if job['url']:
                            f.write(f"  - [–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é]({job['url']})\n")
                        f.write("\n")
                else:
                    f.write("**–í–∞–∫–∞–Ω—Å–∏–∏:** –ù–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç\n\n")
                    f.write("*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–º–ø–∞–Ω–∏—è –Ω–µ –ø—É–±–ª–∏–∫—É–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —á–µ—Ä–µ–∑ LinkedIn –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Jobs –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.*\n\n")
            else:
                f.write(f"**–û—à–∏–±–∫–∞:** {result.get('error', '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é')}\n\n")
            
            f.write("---\n\n")
        
        # Summary
        total_jobs = sum(r.get('jobs_count', 0) for r in all_results if r.get('success'))
        f.write(f"## –ò—Ç–æ–≥–æ\n\n")
        f.write(f"- **–ö–æ–º–ø–∞–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ:** {len(all_results)}\n")
        f.write(f"- **–í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ:** {total_jobs}\n")
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
    print(f"\nüìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {sum(r.get('jobs_count', 0) for r in all_results if r.get('success'))}")
