#!/usr/bin/env python3
"""
–ó–∞–ø—É—Å–∫ AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ (–æ–±—Ö–æ–¥, –µ—Å–ª–∏ MCP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω).
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –∏–∑ –∫–æ—Ä–Ω—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è Dex:
  python3 .scripts/ai-digest-run.py [hours]
  –Ω–∞–ø—Ä–∏–º–µ—Ä: python3 .scripts/ai-digest-run.py 24
"""
import asyncio
import re
import sys
from datetime import datetime, timezone, timedelta
from pathlib import Path

# add project root
root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(root))

async def main():
    try:
        import feedparser
        import aiohttp
    except ImportError:
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install -r core/mcp/requirements-ai-updates.txt")
        sys.exit(1)
    ssl_ctx = None
    try:
        import ssl
        import certifi
        ssl_ctx = ssl.create_default_context(cafile=certifi.where())
    except ImportError:
        pass

    hours = int(sys.argv[1]) if len(sys.argv) > 1 else 24
    hours = max(1, min(168, hours))
    since = datetime.now(timezone.utc) - timedelta(hours=hours)

    # blog.google/feed –æ—Ç–¥–∞—ë—Ç 0 –∑–∞–ø–∏—Å–µ–π (—Ñ–æ—Ä–º–∞—Ç –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è); Gemini –µ—Å—Ç—å –≤ Google Cloud
    feeds = [
        ("OpenAI", "https://openai.com/blog/rss.xml"),
        ("Google Cloud", "https://cloudblog.withgoogle.com/rss/"),
    ]

    async def fetch(url):
        try:
            connector = aiohttp.TCPConnector(ssl=ssl_ctx) if ssl_ctx else None
            async with aiohttp.ClientSession(connector=connector) as s:
                async with s.get(url, timeout=aiohttp.ClientTimeout(total=20)) as r:
                    if r.status != 200:
                        return [], str(r.status)
                    return feedparser.parse(await r.text()).entries, None
        except Exception as e:
            return [], str(e)

    def parse_date(e):
        if e.get("published_parsed"):
            try:
                return datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
            except Exception:
                pass
        if e.get("published"):
            try:
                from email.utils import parsedate_to_datetime
                return parsedate_to_datetime(e.published)
            except Exception:
                pass
        return None

    def plain_summary(raw, max_len=280):
        if not raw or not raw.strip():
            return ""
        import re
        from html import unescape
        text = unescape(raw)
        text = re.sub(r"<[^>]+>", " ", text)
        text = re.sub(r"\s+", " ", text).strip()
        if len(text) > max_len:
            text = text[: max_len - 3].rsplit(" ", 1)[0] + "..."
        return text

    all_entries = []
    for name, url in feeds:
        entries, err = await fetch(url)
        if err:
            all_entries.append({"title": f"–û—à–∏–±–∫–∞: {err}", "link": url, "published": None, "source": name, "summary": ""})
            continue
        for e in entries:
            title = (e.get("title") or "").strip()
            link = (e.get("link") or "").strip()
            pub = parse_date(e)
            summary_raw = (e.get("summary") or e.get("description") or "")[:500].strip()
            all_entries.append({"title": title, "link": link, "published": pub, "source": name, "summary": summary_raw})
        src = [x for x in all_entries if x["source"] == name]
        src.sort(key=lambda x: x["published"] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)
        all_entries = [x for x in all_entries if x["source"] != name] + src[:30]

    in_period = [e for e in all_entries if e.get("published") and e["published"] >= since]
    in_period.sort(key=lambda x: x.get("published") or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

    now = datetime.now(timezone.utc)
    lines = [f"# AI –¥–∞–π–¥–∂–µ—Å—Ç ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours} —á (–¥–æ {now.strftime('%Y-%m-%d %H:%M')} UTC)\n"]
    by = {}
    for e in in_period:
        by.setdefault(e["source"], []).append(e)
    for src in sorted(by.keys()):
        lines.append(f"\n## {src}\n")
        for e in by[src][:20]:
            pub = (e["published"].strftime("%Y-%m-%d") if e.get("published") else "")
            lines.append(f"- **[{e['title'] or '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'}]({e['link']})**" + (f" ‚Äî {pub}" if pub else ""))
            s = plain_summary(e.get("summary") or "")
            if s:
                lines.append(f"  {s}")

    summary = "\n".join(lines).strip()
    print(f"ü§ñ AI –î–ê–ô–î–ñ–ï–°–¢ ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours} —á (–¥–æ {since.isoformat()} UTC)\n")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(in_period)}\n")
    print(summary)


if __name__ == "__main__":
    asyncio.run(main())
